{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from scipy.stats import laplace, norm, t\n",
    "import scipy\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANCE = 2.0\n",
    "\n",
    "normal_scale = math.sqrt(VARIANCE)\n",
    "student_t_df = (2 * VARIANCE) / (VARIANCE - 1)\n",
    "laplace_scale = VARIANCE / 2\n",
    "\n",
    "HYPOTHESIS_SPACE = [norm(loc=0.0, scale=normal_scale),\n",
    "                    laplace(loc=0.0, scale=laplace_scale),\n",
    "                    t(df=student_t_df)]\n",
    "\n",
    "PRIOR_PROBS = np.array([0.35, 0.25, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n_samples, seed=None):\n",
    "    \"\"\" data generating process of the Bayesian model \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    hypothesis_idx = np.random.choice(3, p=PRIOR_PROBS)\n",
    "    dist = HYPOTHESIS_SPACE[hypothesis_idx]\n",
    "    return dist.rvs(n_samples, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.90455498, -0.15186788,  0.82050848])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Solution \"\"\"\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "def log_posterior_probs(x):\n",
    "    \"\"\"\n",
    "    Computes the log posterior probabilities for the three hypotheses, given the data x\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): one-dimensional numpy array containing the training data\n",
    "    Returns:\n",
    "        log_posterior_probs (np.ndarray): a numpy array of size 3, containing the Bayesian log-posterior probabilities\n",
    "                                          corresponding to the three hypotheses\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "\n",
    "    # Compute log-likelihood for each hypothesis\n",
    "    log_likelihoods = np.array([\n",
    "        np.sum(HYPOTHESIS_SPACE[0].logpdf(x)),  # Normal distribution\n",
    "        np.sum(HYPOTHESIS_SPACE[1].logpdf(x)),  # Laplace distribution\n",
    "        np.sum(HYPOTHESIS_SPACE[2].logpdf(x))   # Student-t distribution\n",
    "    ])\n",
    "\n",
    "    # Compute log-prior probabilities (log of PRIOR_PROBS)\n",
    "    log_priors = np.log(PRIOR_PROBS)\n",
    "\n",
    "    # Compute unnormalized log-posteriors: log P(H_i | X) = log P(X | H_i) + log P(H_i)\n",
    "    log_p = log_likelihoods + log_priors\n",
    "\n",
    "    # Normalize using logsumexp to get log P(X)\n",
    "    log_p -= logsumexp(log_p)\n",
    "\n",
    "    assert log_p.shape == (3,)\n",
    "    return log_p\n",
    "\n",
    "\n",
    "def posterior_probs(x):\n",
    "    return np.exp(log_posterior_probs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior probs for 1 sample from Laplacian\n",
      "Normal: 0.3239 , Laplace: 0.2441, Student-t: 0.4320\n",
      "\n",
      "Posterior probs for 50 samples from Laplacian\n",
      "Normal: 0.2550 , Laplace: 0.3388, Student-t: 0.4062\n",
      "\n",
      "Posterior probs for 1000 samples from Laplacian\n",
      "Normal: 0.0000 , Laplace: 1.0000, Student-t: 0.0000\n",
      "\n",
      "Posterior for 100 samples from the Bayesian data generating process\n",
      "Normal: 0.0513 , Laplace: 0.0381, Student-t: 0.9106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" sample from Laplace dist \"\"\"\n",
    "    dist = HYPOTHESIS_SPACE[1]\n",
    "    x = dist.rvs(1000, random_state=28)\n",
    "\n",
    "    print(\"Posterior probs for 1 sample from Laplacian\")\n",
    "    p = posterior_probs(x[:1])\n",
    "    print(\"Normal: %.4f , Laplace: %.4f, Student-t: %.4f\\n\" % tuple(p))\n",
    "\n",
    "    print(\"Posterior probs for 50 samples from Laplacian\")\n",
    "    p = posterior_probs(x[:50])\n",
    "    print(\"Normal: %.4f , Laplace: %.4f, Student-t: %.4f\\n\" % tuple(p))\n",
    "\n",
    "    print(\"Posterior probs for 1000 samples from Laplacian\")\n",
    "    p = posterior_probs(x[:1000])\n",
    "    print(\"Normal: %.4f , Laplace: %.4f, Student-t: %.4f\\n\" % tuple(p))\n",
    "\n",
    "    print(\"Posterior for 100 samples from the Bayesian data generating process\")\n",
    "    x = generate_sample(n_samples=100)\n",
    "    p = posterior_probs(x)\n",
    "    print(\"Normal: %.4f , Laplace: %.4f, Student-t: %.4f\\n\" % tuple(p))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvhr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
